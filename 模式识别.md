# 模式识别

## 贝叶斯决策理论

### 基本符号与定义

**决策**：根据观测到的x，利用先验和类条件概率决定x属于哪一类。

**Bayes决策:**
$$
p(\omega_i |x)=\frac{p(x|\omega_i)p(\omega_i)}{p(x)}
$$
**决策规则**
$$
p(\omega_i|x) = \max_{j=1,2}p(\omega_j|x),x\in \omega_i
$$
等价形式：
$$
p(x|\omega_i)p(\omega_i) = \max_{j=1,2}p(x|\omega_j)p(\omega_i)
$$


$$
若l(x)=\frac{p(x|\omega_1)}{p(x|\omega_2)}< \frac{p(\omega_2)}{p(\omega_1)},则x\in\frac{\omega_1}{\omega_2}
$$

#### 基于最小错误

错误率：
$$
p(e) = \int_{-\infin}^{+\infin}p(e,x)\mathrm dx = \int_{-\infin}^{+\infin}p(e|x)p(x)\mathrm dx
$$

#### 基于最小风险

引入风险函数（损失函数）$\lambda(x)$

风险损失：采用决策$a_i$时的风险
$$
R(a_i|x)=E(\lambda(a_i,\omega_j))=\sum_{j=1}^c\lambda(a_i,\omega_j)p(\omega_j|x)
$$

### 多类情况贝叶斯决策规则

#### 判别函数

$g_i(x)$，一般选取$g_i(x)=\max_jg_j(x)$，则将$x$归于$\omega_i$类。

#### 决策面方程

$$
g_i(x) = g_j(x)
$$

![image-20210914102326475](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20210914102326475.png)

### 正态分布统计决策

最小错误率贝叶斯判别函数：
$$
g_i(x) = \ln p(x|\omega_i) + \ln p(\omega_i)
$$
带入正态分布：
$$
g_i(x) = -0.5(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i)-\frac{d}{2}\ln2\pi-0.5\ln|\Sigma_i|+\ln p(\omega_i)
$$
决策面方程：
$$
-0.5[(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i)-(x-\mu_j)^T\Sigma_j^{-1}(x-\mu_j)]-0.5\ln \frac{|\Sigma_i|}{|\Sigma_j|}+\ln\frac{p(\omega_i)}{p(\omega_j)}=0
$$
**几个特殊情况：**

1. $\Sigma_i=\sigma^2I$，则判别函数为：$-\frac{1}{2\sigma^2}(x-\mu_i)^T(x-\mu_i)+\ln p(\omega_i)$。若先验概率相等，直接等价于最小距离分类器。![image-20210914105907548](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20210914105907548.png)
2. 每一个协方差矩阵都相等，则判别函数为：$-\frac{1}{2\sigma^2}(x-\mu_i)^T(x-\mu_i)-\frac{d}{2}\ln 2\pi-0.5\ln \sigma^{2d}+\ln p(\omega_i)$式中$d$为维度。可简化为：$-\frac{1}{2\sigma^2}(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i)+\ln p(\omega_i)$![image-20210914105915313](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20210914105915313.png)
3. 若协方差矩阵各不相等：![image-20210914105955625](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20210914105955625.png)

## 概率密度函数的估计

### 基于样本的两步贝叶斯决策

- 利用样本集估计$P(\omega_i)$和$P(x|\omega_i)$
- 得到$\hat P(\omega_i)$和$\hat P(x|\omega_i)$
- 将估计量带入贝叶斯决策规则
- 得到决策结果

首先通过训练样本估计概率密度函数

- 先验概率估计-训练样本分布情况/根据领域知识认定
- 但类条件概率密度估计难

统计决策进行类别判定

- 训练样本有限，难以涵盖所有情况
- 但当训练样本多，就可以趋近于理论贝叶斯决策

### 概率密度估计方法

由训练样本集估计总体概率密度的方法可分为：

- 监督参数估计
- 非监督参数估计
- 非参数估计

### 参数估计的基本概念

统计量、参数空间

点估计：构造一个统计量作为某参数的估计

估计量、估计值、区间估计

### 概率密度估计的评估

无偏性：$\mathbb E\theta = \hat \theta$

渐进无偏：$N$趋于无穷有无偏性

有效性：一种估计比另一种方差小

一致估计
$$
\lim _{n\to\infin}p(|\hat{\theta_n}-\theta|>\epsilon) = 0
$$
则$\hat \theta$是$\theta$的一致估计

### 最大似然估计

#### 基本假设

- $\theta$确定但未知
- 按类别把样本集分开，$\Re_j$类中每个样本都是对立从概率密度$p(x|\omega_i)$的总体中独立抽取出来的。每即个样本i.i.d
- $p(x|\omega_j)$为已知分布，参数向量未知，且每个不同类别参数在函数上独立

#### 似然函数定义

在一类中独立抽取样本集来估计未知参数。

假设某类样本集中有$N$个样本
$$
\Re=\{x_1,x_2,\dots,x_N\}
$$
因样本独立抽取，样本出现在样本集中的概率
$$
l(\theta) = p(\Re|\theta)=p(x_1,x_2,\dots,x_N|\theta)=p(x_1|\theta)p(x_2|\theta)\dots p(x_N|\theta)
$$

也可以取对数。

#### 求解

由于每个训练样本独立，可对概率乘积取对数，再求导。
$$
\forall i,\frac{\part}{\part \theta_i}\sum_{k=1}^N\log p(x_k|\theta^i) = 0
$$
注意：方程有可能有多解，但只有一个解最大。

#### 多维正态分布的最大似然估计

##### $\Sigma$已知，$\mu$未知，求$\mu$

$$
\mu = \hat \mu = \frac 1 N\sum_{k=1}^N x_k
$$

##### $\Sigma,\mu$均未知

$$
\hat \theta_1=\hat\mu_1=\frac 1 N \sum_{k=1}^Nx_k
$$

$$
\hat \theta_2=\hat  \sigma^2_1=\frac 1 N\sum_{k=1}^N(x_k-\hat\mu)^2
$$

### 贝叶斯估计和贝叶斯学习

贝叶斯估计实质：贝叶斯决策来决策参数的取值。

与最大似然估计的区别：

- 最大似然估计把待估计的参数当作未知但固定的量
- 贝叶斯估计把待估计的参数也看为随机变量

贝叶斯学习：把贝叶斯估计的原理用于直接从数据对概率密度函数进行迭代估计。

可以回顾[最小风险贝叶斯决策](#基于最小风险)

#### 贝叶斯估计量

$R(\hat\theta|x)$为给定$x$条件下估计量$\hat\theta$的期望损失，称为条件风险

定义：如果$\theta$的估计量$\hat \theta$使得条件风险最小，则为贝叶斯估计量。

#### 损失函数

损失函数有多种，最常见为平方误差：
$$
\lambda(\hat\theta,\theta)=(\hat\theta-\theta)^2
$$

#### 贝叶斯估计

定理：如果损失函数为二次函数，则贝叶斯估计量为给定$x$时$\theta$的条件期望：
$$
\hat\theta=\mathbb E[\theta|x] = \int_\Theta\theta p(\theta|x)\mathrm d\theta
$$

#### 估计基本步骤

1. 确定$\theta$的先验分布$p(\theta)$，待估参数为随机变量
2. 用第$i$类样本$X^i=(X_1,X_2,\dots，X_N)$求出样本的联合概率密度分布$p(X^i|\theta)$，是一个$\theta$的函数
3. 利用贝叶斯公式，求$\theta$的后验概率

$$
p(\theta|X^i)=\frac{p(X^i|\theta)p(\theta)}{\int p(X^i|\theta)p(\theta)\mathrm d \theta}
$$

4. 求贝叶斯估计

$$
\hat \theta=\int_\Theta\theta p(\theta|X^i)\mathrm d\theta
$$

这两种参数估计方法，最终目的是估计总体分布
$$
p(x|\Re) \ \Re\to X^i
$$
其实在第三步后可以直接通过联合密度求类条件概率密度
$$
p(x|X^i)\int p(x,\theta|X^i)\mathrm d\theta=\int p(x|\theta)p(\theta|X^i)\mathrm d\theta
$$

#### 正态分布的均值估计

一维正态分布，已知$\sigma^2$，估计$\mu$。

假设概率密度服从正态分布，则$p(X|\mu)\sim N(\mu,\sigma^2),p(\mu)\sim N(\mu_0,\sigma^2)$

用第$i$类样本$X^i=(X_1,X_2,\dots,X_N)$，求出后验概率：
$$
p(\mu|X^i)=\frac{p(X^i|\mu)p(\mu)}{\int p(X^i|\mu)p(\mu)\mathrm d\mu}
$$
因为$N$个样本独立抽取，且$\int p(X^i|\mu)p(\mu)\mathrm d\mu$仅仅与$x$有关，则上式可改写为：
$$
\begin{align}
p(\mu|X^i)&=a\prod_{k=1}^Np(x_k|\mu)p(\mu)\\
&=a\prod_{k=1}^N\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac 1 2 (\frac{x_k-\mu}{\sigma})^2\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac 1 2 (\frac{\mu-\mu_0}{\sigma} )^2))\\
&=a^*\exp(-\frac 1 2(\sum_{k=1}^N(\frac{x_k-\mu}{\sigma})^2+(\frac{\mu-\mu_0}{\sigma} )^2))\\
&=a^{**}
\end{align}
$$


#### 非监督参数估计

在未知样本类别的条件下的参数估计称为**非监督参数估计**

几个基本假设

- 样本来自类数为$c$的各类中，但不知道每个样本究竟来自于哪一类
- 每类的先验概率$p(\omega_j)$已知
- 类条件概率密度的形式$p(x|\omega_j,\theta_j)$已知
- 未知的只是$c$个参数向量$\theta_1,\theta_2,\dots,\theta_c$的值

似然函数：
$$
l(\theta)=p(\Re|\theta)
$$

## 线性判别函数

### 引言

基于样本的Bayes分类器：通过估计类条件概率密度，设计相应判别函数

如果可以估计概率密度函数，则可以使用贝叶斯决策来最优的实现分类

从样本数据中直接估计参数->根据对样本/问题的理解直接设定判别函数形式，直接求解！

![image-20211011155524302](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20211011155524302.png)

#### 参数方法

概率密度函数已知，训练样本去估计参数

#### 线性判别方法

与非参数方法类似，与参数没有直接关系。次优方法。

#### 基于样本的直接确定判别函数的方法

1. 针对不同情况，使用不同准则函数，设计出满足这些不同准则要求的分类器。
2. 这些准则的“最优”并不一定与错误率最小相一致：次优分类器。

### 线性判别函数的基本概念

d维空间中线性判别函数的一般形式：
$$
g(x) = w^Tx+w_0
$$
其中：$x$是样本向量-样本在$d$维特征空间中的描述，$w$是权向量，$w_0$是一个常数。

令$g(x) = g(x_1)-g(x_2)$，$g(x) = 0$定义了一个决策面，分开了两类。

判别函数$g(x)$也可以看作是特征空间中某个点$x$到超平面距离的一种代数度量。可定义：
$$
x=x_p+r\frac{w}{||w||}
$$
其中$x_p$是$x$到$H$的投影向量，$r$是$x$到$H$的垂直距离。

则
$$
\begin{align}
g(x) &=w^T(x_p+r\frac{w}{||w||})+w_0\\
&=w^Tx_p+w_0+r\frac{w^Tw}{||w||}\\
&=r||w||
\end{align}
$$
特殊情况：x为原点，则$g(x)=w_0$。

#### 设计分类器的主要步骤

1. 有一组具有类别标志的样本集
2. 根据实际情况确定一个准则函数 $J$,满足：$ J$是样本集和$w,wo ,a$的函数；$J$的值能反映分类器的性能，它的极值解对应于 “最好”的决策
3. 利用最优化方法求出准则函数的极值解和$w,wo ,a$，进而得到$g(x)$

### Fisher线性判别分析

#### 基本思想

希望投影后的一维数据满足：

1. 两类之间尽可能远
2. 每一类自身尽可能紧凑

即：用投影后数据的统计性质（均值和离散度）的函数作为判别优劣的标准

#### 定义符号

$m_1,m_2$两类数据均值向量

$S_1,S_2$两类数据离散度矩阵

$\mu_1,\mu_2$两类数据投影后一维数据均值

$\sigma_1,\sigma_2$两类数据投影后一维数据离散度
$$
m_i=\frac 1 N\sum^i x
$$

$$
S_i=\sum^i((x-m_i)(x-m_i)^T)
$$
则有：
$$
\begin{align}
\mu_i &= w^Tm_i\\
\sigma^2_i&=\sum(w^Tx-\mu_i)^2\\
&=w^T\sum(x-m_i)(x-m_i)^Tw\\
&=w^TS_iw
\end{align}
$$

#### Fisher准则函数

$$
J_F(w)=\frac{(\mu_1-\mu_2)^2}{\sigma_1^2+\sigma_2^2}\\
w_{opt} = \arg\max J_F(w)
$$

称$S_b=(m_1-m_2)(m_1-m_2)^T$类间离散度矩阵

称$S_t=S_1+S_2$类内总离散度矩阵

则：
$$
J_F(w)=\frac{w^TS_bw}{w^TS_tw}
$$

#### Fisher准则合理性

$J_F(w)$只与投影方向有关，与$||w||$无关，若$w$是最优解，则$\lambda w$也是最优解。

#### Fisher最佳投影方向求解

需要$S_t=S_1+S_2$正定，否则存在$w$使得$w^TS_tw=0$，导致$J_F(w)$无极大值。

又因为有界性，则$J_F(w)\le\frac{\max \lambda(S_b)}{\min \lambda(S_t)}$

$\lambda(S)$表示$S$的特征根。

又因为$S_t$正定，则存在最优的$w$，使得$w^TS_tw=1$

本来是无约束优化$\max\frac{w^TS_bw}{w^TS_tw}$，等价为带约束最优化：
$$
\max w^tS_bw\\
s.t.\space w^TS_tw=1
$$

$$
L(w,\lambda) = w^TS_bw-\lambda(w^TS_tw-1)\\
\frac{\part L(w,\lambda)}{\part w} = S_bw-\lambda S_tw =0
$$
由定义：
$$
(m_1-m_2)(m_1-m_2)^Tw_{opt}=\lambda S_tw_{opt}
$$
记$c=(m_1-m_2)^Tw_{opt}$，则：
$$
w_{opt}=\frac{c}{\lambda}S_t^{-1}(m_1-m_2)
$$
而我们只关心投影方向，所以：
$$
w_{opt}=S_t^{-1}(m_1-m_2)=(S_1+S_2)^{-1}(m_1-m_2)
$$
映射就此完成，现在确定分类阈值$w_0$，即两个样本投影后的加权平均值（权为样本个数）

### 感知准则函数

为讨论方便，将$x$增加一维：$y=[1,x_1,x_2,\dots,x_d]^T$，增广的权向量为：$a=[w_0,w_1,w_2,\dots,w_d]^T$ ，线性判别函数为：$g(y)=a^Ty$

#### 基本概念

##### 线性可分性

训练样本集中的两类样本在特征空间可以用一个线性分界面正确无误的分开。

##### 规范化样本向量

将第二类样本取其反向向量。

![image-20211011164033943](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20211011164033943.png)

#### 感知准则函数

$$
J_P(a)=\sum_{y\in Y_M}(-a^Ty)
$$

式中$Y_M$是被$a$错误分类的实例集合，由于全部反向，导致所有错误分类的实例有$a^Ty<0$，因此$J_P(a)$非负。

定义梯度：
$$
\nabla_aJ_P(a)=\sum_{y\in Y_M}(-y)
$$
梯度衰减更新法则为：
$$
a_{k+1} = a_k+\eta_k\sum_{y\in Y_M(k)}y
$$
也被称为感知机批次更新法则。

![image-20211011164542002](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20211011164542002.png)

感知准则函数修正法：

1. 单样本修正法：样本集视为不断重复出现的序列，逐个样本检查，修正权向量
2. 批量样本修正法：样本成批或全部检查后，修正权向量。

#### 小结

感知准则函数方法思路：

1. 找初始向量$a_1$，用训练样本集中每个样本计算
2. 若发现某个$y$有$a^Ty<0$，则只要$a_{k+1} = a_k+\eta_ky$，则必有$a_{k+1}^Ty = a_k^Ty+\eta_ky^Ty$，有趋势使得$a_{k+1}^Ty>0$

当然，修正后的$a$也有可能使某些$y$出现$a_{k+1}^Ty<0$，但只要训练样本集线性可分，无论初值为什么，有限次迭代都能收敛。

### 最小平方误差准则函数

规范化增广样本向量$y_i$，增广权向量$a$，正确分类要求：$a^Ty_i>0$

本质就是求一组$N$个线性不等式的解。

样本集增广矩阵$Y$及一组$N$个线性不等式可由矩阵表示

引入余量$b=[b_1,b_2,\dots,b_N]^T$，$b_i$为任意给定正常数，$a^Ty_i=b_i>0$，则可表示为：
$$
Ya=b
$$
定义误差向量$e=Ya-b$，定义平方误差准则函数：
$$
J_S(a)=||e||^2=||Ya-b||^2=\sum_{i=1}^N(a^Ty_i-b_i)^2
$$
求解
$$
a^*=\arg\min_aJ_S(a)
$$

$$
\nabla J_S(a)\sum_{i=1}^N2(a^Ty_i-b_i)y_i=2Y^T(Ya-b)
$$

则：
$$
\nabla J_S(a^*)=0 \iff Y^TYa^*=Y^Tb\\
a^*=(Y^TY)^{-1}Y^Tb=Y^+b
$$

#### 与其他方法的关系

与Fisher：当$b=[N/N_1,N/N_1,\dots,N/N_1(N_1个),N/N_2,N/N_2,\dots,N/N_2(N_2个)]^T$

MSE等价于Fisher

与Bayes：当$N\to \infin,b=u_N$时，则它以最小均方误差逼近Bayes判别函数$g(x)=P(\omega_1|x)-P(\omega_2|x)$

#### MSE方法的迭代解

伪逆解计算量大，实际使用的梯度下降法
$$
\nabla J_S(a)\sum_{i=1}^N2(a^Ty_i-b_i)y_i=2Y^T(Ya-b)=\left\{\begin{align}&a_1随机初始化\\&a_{k+1}=a_k-\eta_kY^T(Ya-b) \end{align} \right.
$$
满足一定条件时梯度下降结束。

也可以使用单样本修正调整权向量：Widrow-Hoff/最小均方根/LMS算法
$$
a_{k+1} = a_k +\eta_k(b_i-a_K^Ty_i)y_i
$$
其中$y_i$是使得$a_K^Ty_i\ne b_i$的样本。

### 多类问题

三种方法：

#### c-1 二类问题

分离符合与不符合$\omega_1$的点

#### c(c-1)/2 二类问题

分离每一对

#### 定义c个线性离散函数

将$x$归于$w_j$类如果$g_i(x)>g_j(x),\forall j\ne i$
$$
g_i(x) = w^T_ix+w_{i0}
$$

## 非线性判别函数

### 引言

线性判别函数：简单实用，但线性不可分时错误率大。

1. 使用新的特征
2. 非线性变换
3. 非线性分类器

### 分段线性判别函数

- 决策面由若干超平面段组成，计算相对比较简单
- 能够逼近各种形状的超平面，适应性强
- 多类情况下的线性判别函数分类
- 树状分类

如果两类可划分为线性可分的若干子类，则可以设计多个线性分类器，实现分段线性分类器。

#### 基于距离的分段线性判别函数

分段线性距离分类器：将各类别划分为相对密集的子类，每个子类以它们的均值作为代表点，然后按最小距离分类。

判别函数：$\omega_i$有$l_i$个子类，将$\omega_i$的决策域$R_i$分成$l_i$个子域$R_i^1,R_i^2,\dots,R_i^{l_i}$，每个区域用均值$m_i^k$代表：
$$
g_i(x)=\min_{k=1,\dots,l_i}||x-m_i^k||
$$
判别规则：
$$
j=\arg\min_{i=1,\dots,c}g_i(x)
$$
如果$g_j(x)=\arg\min_{i=1,\dots,c}g_i(x)$，则$x$属于$\omega_j$。

#### 一般的分段线性判别函数

将每个大类分成若干子类，针对每个子类定义一个线性判别函数。

一般形式：$g_i^k(x)$表示第$i$类第$k$段线性判别函数，$l_i$是第$i$类所具有的判别函数个数，$w_i^k.w_{i_0}^k$扽别是第$k$段的权向量和阈值权：
$$
g_i^k(x)={w_i^{(k)}}^Tx+w_{i_0}^k
$$
第$i$类判别函数：
$$
g_i(x)-\max_{k=1,2,\dots,l_i}g_i^k(x)
$$
判别规则：如果$g_j(x)=\arg\min_{i=1,\dots,c}g_i(x)$，则$x$属于$\omega_j$。

决策面：$g_i^n(x)=g_j^m(x)$

问题：如何确定子类数目？如何求得各子类的线性判别函数？

### 分段线性判别函数的设计

##### 已知子类划分

直接使用多类线性分类方法

如何知道子类划分？**先验、聚类分析**

##### 只知道子类数目，不知道子类划分

使用错误修正法（此法介绍使用增广的判别函数形式表示$g_i^k(y)={a_i^{(k)}}^Ty$）

假设$\omega_i$，$\omega_i$类中有$l_i$个子类，每一类均存在一定数量训练样本：

1. 初始化：任意给定各类各子类权值${a_i^{(k)}}^T(0)$，通常使用小随机数
2. 在时刻$t$：当前权值为${a_i^{(k)}}^T(t)$，考虑某个训练样本$y_v\in\omega_j$，找出$\omega_j$类的子类中最大的判别函数${a_i^{(m)}}^T(t)y_v=\max_k{a_j^{(k)}}^T(t)y_v$
3. 考察当前权值对$y_v$的分类情况，若${a_j^{(m)}}^T(t)y_v>{a_i^{(k)}}^T(t)y_v,\forall i\ne j$，则${a_i^{(k)}}^T(t)$不变。若$\exist i\ne j,k=n,s.t.{a_j^{(m)}}^T(t)y_v\le{a_i^{(n)}}^T(t)y_v$，则$y_v$被错分，对其中最大者记为$(i^{'},n^{'})$修正：

$$
a_j^{(m)}(t+1) = a_j^{(m)}(t) + \rho_ty_t\\
a_{i^{'}}^{(n^{'})}(t+1) = a_{i^{'}}^{(n^{'})}(t) - \rho_ty_t
$$

重复迭代，直至收敛。式中$\rho$为自取步长

##### 未知子类数目

使用树状分段线性分类器。

#### 用凹函数的并来表示分段线性函数

设$l_i$为线性判别函数，则：

1. $l_1,l_2,\dots,l_r$都是分段线性判别函数
2. 若$A,B$都是分段线性判别函数，则：$A\wedge B,A\vee B$也是分段线性判别函数。
3. 对任何分段线性函数都可以表示为析取范式或合取范式。

析取范式中最小项$(L_{11}\wedge L_{22}\wedge\dots\wedge L_{1m})$称为**凹函数**。

对于多峰二类问题：设第一类有$q$个峰，则有$q$个凹函数，即：
$$
P=P_1\vee P_2 \vee \dots \vee P_q
$$
每个凹函数$P_i$由$m$个线性判别函数来构成：
$$
P_i=L_{i1}\wedge L_{i2}\wedge \dots \wedge L_{im}
$$
假设对于每个子类的线性判别函数$L_{ij}$都设计成：
$$
L_{ij}=w_{ij}x\left\{ \begin{aligned}&>0,x\in\omega_1,i=1,2,\dots,q子类\\&<0,x\in\omega_2,j=1,2,\dots,m每个子类的判别函数数\end{aligned}\right.
$$
最终判别规则：
$$
P>0,则x\in\omega_1\\P\le0,则x\in\omega_2
$$
例：

![image-20211018165243575](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20211018165243575.png)

$\omega_1$分三个峰，$q=3$。

判别函数：
$$
m_1=5,m_2=4,m_3=4
$$
所以共有十三个分段判别函数
$$
\begin{align}
P&=(L_{11}\wedge L_{12}\wedge L_{13}\wedge L_{14}\wedge L_{15})\vee(L_{21}\wedge L_{22}\wedge L_{23}\wedge L_{24})\vee(L_{31}\wedge L_{32}\wedge L_{33}\wedge L_{34})\\
&=\max(\min(l_{11},l_{12},l_{13},l_{14},l_{15}),\min(l_{21},l_{22},l_{23},l_{24}),\min(l_{31},l_{32},l_{33},l_{34}))
\end{align}
$$
若$P>0$则$x\in\omega_1$，否则$x\in \omega_2$

#### 用交遇区的样本设计分段线性函数

思想：寻找两类中最靠近的样本子集，用他们设计分类器。

步骤：

1. 用聚类分析等方法把每类样本分为若干子类
2. 考察子类间的距离$d(v_i^m,v_j^n)$
3. 寻找紧互对原型对$d(v_i^m,v_j^n)=\min_l(d(v_i^l,v_j^n))=\min_l(d(v_i^m,v_j^l))$
4. 用紧互对原型对设计分类面
5. 决策规则（可能有错分）：设最后得到$m$个超平面$H_i:\alpha _i^Ty=0$，记$z_i(x)=if\ \alpha _i^Ty>0: 1,\ else\ 0$，得$z(x)=[z_1(x),z_2(x),\dots,z_m(x)]$，对$z(x)$的每一种可能取值$z_j$，统计其在$\chi_1,\chi_2$两类样本中出现的次数$N_1(z_j),N_2(z_j)$，定义$\Omega(z_j)$：若$N_1(z_j),N_2(z_j)$很小，则$\Omega(z_j)=\delta$，否则若$L=\frac {N_1(z_j)}{N_1(z_j)+N_2(z_j)}>1$，则$\Omega(z_j)=1$，否则若$L<2$，$\Omega(z_j)=0$。
6. 最终决策规则：对输入$x$，若$\Omega(z_j)=1$则$x\in \omega_1$；若$\Omega(z_j)=0$，则$x\in\omega_2$；若$\Omega(z_j)=\delta$则拒绝。

### 二次判别函数

二次判别函数一般表示为：
$$
\begin{align}
g(x)&=X^T\bar WX+W^TX+W_0\\
&=\sum_{i=1}^nw_{ii}x_i^2+2\sum_{j=1}^{n-1}\sum_{i=j+1}^nw_{ji}x_jx_i+\sum_{j=1}^nw_jx_j+W_0
\end{align}
$$
其中：$\bar W$是$n\times n$维权向量，$W$为$n$维权向量，系数共有$l=\frac 1 2n(n+3)+1$（很大）

二次决策面为超二次曲面。

##### 已知样本$\omega_1$集中，而$\omega_2$分散

定义$\omega_1$判别函数：
$$
g(x) = k^2-(x-\bar \mu)^T\Sigma^{-1}(x-\bar \mu)
$$
$k$决定超平面大小，$\mu$为$\omega_1$均值，$\Sigma$为$\omega_1$协方差。

##### 已知样本$\omega_1,\omega_2$都集中

可以定义两个判别函数：
$$
g_i(x) = k_i^2-(x-\bar \mu_i)^T\Sigma_i^{-1}(x-\bar \mu_i)
$$
$\mu_i$为$\omega_i$均值，$\Sigma_i$为$\omega_i$协方差。

判别面方程：
$$
g(x)=g_1(x)-g_2(x) = 0
$$

### 神经网络综述

懒了，摆了。
