# 模式识别

[TOC]

## 绪论

### 模式识别

模：法也；式，法也。

模式：一种规律

Pattern：事物的模板或原型，或者表征事物特点的特征或性状的组合。

模式识别中的模式：

- 存在于时间、空间中可观察的事物，具有时间空间分布的信息。
- 对客体特征的描述。

样本：

- 一个具体的研究对象

模式类：

- 具有某些共同特性的模式的集合

模式识别：

- 确定一个样本的类别属性（模式类）的过程
- 把某一样本归属于多个类型中的某个类型

模式识别——用计算机实现人对各种事物或现象的分析、描述、判断、识别。

### 模式识别系统

#### 典型构成

##### 监督模式识别

回归：

- 反映样本数据集中样本的属性值的特性，通过函数表达样本映射的关系来发现属性值之间的依赖关系。

分类：

- 将样本数据集中的样本映射到某个给定的类别中。

一般步骤：

1. 分析问题
2. 原始特征获取
3. 特征提取与选择
4. 分类器设计
5. 分类决策

##### 非监督模式识别

聚类：

- 将样本数据集中的样本分为几个类别，属于同一类别的样本相似性比较大。

一般步骤：

1. 分析问题
2. 原始特征获取
3. 特征提取与选择
4. 聚类分析
5. 结果解释

##### 比较

有已知样本：监督模式识别

无已知样本：非监督模式识别

## 贝叶斯决策理论

### 基本符号与定义

**决策**：根据观测到的$x$，利用先验和类条件概率决定$x$属于哪一类。决策是从样本空间到决策空间的一个映射。

**Bayes决策**

两种常用的准则：

- 最小错误概率准则
- 最小风险准则

#### 基于最小错误

贝叶斯概率：
$$
p(\omega_i |x)=\frac{p(x|\omega_i)p(\omega_i)}{p(x)}
$$
**决策规则**
$$
p(\omega_i|x) = \max_{j=1,2}p(\omega_j|x),x\in \omega_i
$$
等价形式：
$$
p(x|\omega_i)p(\omega_i) = \max_{j=1,2}p(x|\omega_j)p(\omega_i)
$$


$$
若l(x)=\frac{p(x|\omega_1)}{p(x|\omega_2)}< \frac{p(\omega_2)}{p(\omega_1)},则x\in\frac{\omega_1}{\omega_2}
$$

错误率：
$$
p(e) = \int_{-\infin}^{+\infin}p(e,x)\mathrm dx = \int_{-\infin}^{+\infin}p(e|x)p(x)\mathrm dx
$$

条件错误率$p(e|x)$的计算：

- 以两类问题为例，当获得观测值$x$之后，有两种决策可能：决定$x\in \omega_1$或$x\in\omega_2$。则条件错误率为：

$$
p(e|x)=
\left\{
\begin{aligned}
p(\omega_2|x)=1-p(\omega_1|x)\quad\mathrm{if}\ x\in\omega_1 \\
p(\omega_1|x)=1-p(\omega_2|x)\quad\mathrm{if}\ x\in\omega_2
\end{aligned}\right.
$$

贝叶斯最小错误率决策：

- 选择后验概率$p(\omega_1|x)$，$p(\omega_2|x)$中大的$\omega$作为决策，使得在观测值$x$下的条件错误率最小：

$$
D(x)=\arg\max_ip(\omega_i|x)
$$

条件错误率：
$$
p(e|x)=1-\max_ip(\omega_i|x)
$$
错误率：
$$
p(e)=\mathbb E(p(e|x))
$$
同时可以证明，此决策保证了每个观测值下的条件错误率最小。Bayes决策是一致最优决策。

#### 基于最小风险

引入风险函数（损失函数）$\lambda(x)$

风险损失：采用决策$a_i$时的风险
$$
R(a_i|x)=\mathbb E(\lambda(a_i,\omega_j))=\sum_{j=1}^c\lambda(a_i,\omega_j)p(\omega_j|x)
$$

$x$是随机变量，采用$x$不同的观测值，产生的条件风险不同。决策$a$可以看成$x$的函数。定义期望风险：
$$
R = \int R(a(x)|x)p(x)\mathrm dx
$$
条件风险对应的是$x$，期望风险对应的是$a(x)$。

在考虑错判带来的损失时，我们希望损失最小，如果在采取每个决策都使其条件风险最小，则对所有的$x$做出决策时，其期望风险也必然最小。这样的决策就是最小风险贝叶斯准则。

即：

如果有：
$$
R(a_k|x)=\min_{i=1,2,\dots,a}R(a_i|x)
$$
则有：$a=a_k$

计算步骤：

1. 计算后验概率
2. 根据决策风险表，计算采取$a_i$的条件风险
3. 对$a$个条件风险值进行比较，找出使条件风险最小的决策$a$。

### 多类情况贝叶斯决策规则

#### 判别函数

$g_i(x)$，一般选取$g_i(x)=\max_jg_j(x)$，则将$x$归于$\omega_i$类。

#### 决策面方程

$$
g_i(x) = g_j(x)
$$

![image-20210914102326475](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20210914102326475.png)

### 正态分布统计决策

最小错误率贝叶斯判别函数：
$$
g_i(x) = \ln p(x|\omega_i) + \ln p(\omega_i)
$$
带入正态分布：
$$
g_i(x) = -0.5(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i)-\frac{d}{2}\ln2\pi-0.5\ln|\Sigma_i|+\ln p(\omega_i)
$$
决策面方程：
$$
-0.5[(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i)-(x-\mu_j)^T\Sigma_j^{-1}(x-\mu_j)]-0.5\ln \frac{|\Sigma_i|}{|\Sigma_j|}+\ln\frac{p(\omega_i)}{p(\omega_j)}=0
$$
**几个特殊情况：**

1. $\Sigma_i=\sigma^2I$，则判别函数为：$-\frac{1}{2\sigma^2}(x-\mu_i)^T(x-\mu_i)+\ln p(\omega_i)$。若先验概率相等，直接等价于最小距离分类器。![image-20210914105907548](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20210914105907548.png)
2. 每一个协方差矩阵都相等，则判别函数为：$-\frac{1}{2\sigma^2}(x-\mu_i)^T(x-\mu_i)-\frac{d}{2}\ln 2\pi-0.5\ln \sigma^{2d}+\ln p(\omega_i)$式中$d$为维度。可简化为：$-\frac{1}{2\sigma^2}(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i)+\ln p(\omega_i)$![image-20210914105915313](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20210914105915313.png)
3. 若协方差矩阵各不相等：![image-20210914105955625](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20210914105955625.png)

## 概率密度函数的估计

### 基于样本的两步贝叶斯决策

- 利用样本集估计$P(\omega_i)$和$P(x|\omega_i)$
- 得到$\hat P(\omega_i)$和$\hat P(x|\omega_i)$
- 将估计量带入贝叶斯决策规则
- 得到决策结果

首先通过训练样本估计概率密度函数

- 先验概率估计-训练样本分布情况/根据领域知识认定
- 但类条件概率密度估计难

统计决策进行类别判定

- 训练样本有限，难以涵盖所有情况
- 但当训练样本多，就可以趋近于理论贝叶斯决策

### 概率密度估计方法

由训练样本集估计总体概率密度的方法可分为：

- 监督参数估计
- 非监督参数估计
- 非参数估计

### 参数估计的基本概念

统计量、参数空间

点估计：构造一个统计量作为某参数的估计

估计量、估计值、区间估计

### 概率密度估计的评估

无偏性：$\mathbb E\theta = \hat \theta$

渐进无偏：$N$趋于无穷有无偏性

有效性：一种估计比另一种方差小

一致估计
$$
\lim _{n\to\infin}p(|\hat{\theta_n}-\theta|>\epsilon) = 0
$$
则$\hat \theta$是$\theta$的一致估计

### 最大似然估计

#### 基本假设

- $\theta$确定但未知
- 按类别把样本集分开，$\Re_j$类中每个样本都是对立从概率密度$p(x|\omega_i)$的总体中独立抽取出来的。每即个样本i.i.d
- $p(x|\omega_j)$为已知分布，参数向量未知，且每个不同类别参数在函数上独立

#### 似然函数定义

在一类中独立抽取样本集来估计未知参数。

假设某类样本集中有$N$个样本
$$
\Re=\{x_1,x_2,\dots,x_N\}
$$
因样本独立抽取，样本出现在样本集中的概率
$$
l(\theta) = p(\Re|\theta)=p(x_1,x_2,\dots,x_N|\theta)=p(x_1|\theta)p(x_2|\theta)\dots p(x_N|\theta)
$$

也可以取对数。

#### 求解

由于每个训练样本独立，可对概率乘积取对数，再求导。
$$
\forall i,\frac{\part}{\part \theta_i}\sum_{k=1}^N\log p(x_k|\theta^i) = 0
$$
注意：方程有可能有多解，但只有一个解最大。

#### 多维正态分布的最大似然估计

##### $\Sigma$已知，$\mu$未知，求$\mu$

$$
\mu = \hat \mu = \frac 1 N\sum_{k=1}^N x_k
$$

##### $\Sigma,\mu$均未知

$$
\hat \theta_1=\hat\mu_1=\frac 1 N \sum_{k=1}^Nx_k
$$

$$
\hat \theta_2=\hat  \sigma^2_1=\frac 1 N\sum_{k=1}^N(x_k-\hat\mu)^2
$$

### 贝叶斯估计和贝叶斯学习

贝叶斯估计实质：贝叶斯决策来决策参数的取值。

与最大似然估计的区别：

- 最大似然估计把待估计的参数当作未知但固定的量
- 贝叶斯估计把待估计的参数也看为随机变量

贝叶斯学习：把贝叶斯估计的原理用于直接从数据对概率密度函数进行迭代估计。

可以回顾[最小风险贝叶斯决策](#基于最小风险)

#### 贝叶斯估计量

$R(\hat\theta|x)$为给定$x$条件下估计量$\hat\theta$的期望损失，称为条件风险

定义：如果$\theta$的估计量$\hat \theta$使得条件风险最小，则为贝叶斯估计量。

#### 损失函数

损失函数有多种，最常见为平方误差：
$$
\lambda(\hat\theta,\theta)=(\hat\theta-\theta)^2
$$

#### 贝叶斯估计

定理：如果损失函数为二次函数，则贝叶斯估计量为给定$x$时$\theta$的条件期望：
$$
\hat\theta=\mathbb E[\theta|x] = \int_\Theta\theta p(\theta|x)\mathrm d\theta
$$

#### 估计基本步骤

1. 确定$\theta$的先验分布$p(\theta)$，待估参数为随机变量
2. 用第$i$类样本$X^i=(X_1,X_2,\dots，X_N)$求出样本的联合概率密度分布$p(X^i|\theta)$，是一个$\theta$的函数
3. 利用贝叶斯公式，求$\theta$的后验概率

$$
p(\theta|X^i)=\frac{p(X^i|\theta)p(\theta)}{\int p(X^i|\theta)p(\theta)\mathrm d \theta}
$$

4. 求贝叶斯估计

$$
\hat \theta=\int_\Theta\theta p(\theta|X^i)\mathrm d\theta
$$

这两种参数估计方法，最终目的是估计总体分布
$$
p(x|\Re) \ \Re\to X^i
$$
其实在第三步后可以直接通过联合密度求类条件概率密度
$$
p(x|X^i)\int p(x,\theta|X^i)\mathrm d\theta=\int p(x|\theta)p(\theta|X^i)\mathrm d\theta
$$

#### 正态分布的均值估计

一维正态分布，已知$\sigma^2$，估计$\mu$。

假设概率密度服从正态分布，则$p(X|\mu)\sim N(\mu,\sigma^2),p(\mu)\sim N(\mu_0,\sigma^2)$

用第$i$类样本$X^i=(X_1,X_2,\dots,X_N)$，求出后验概率：
$$
p(\mu|X^i)=\frac{p(X^i|\mu)p(\mu)}{\int p(X^i|\mu)p(\mu)\mathrm d\mu}
$$
因为$N$个样本独立抽取，且$\int p(X^i|\mu)p(\mu)\mathrm d\mu$仅仅与$x$有关，则上式可改写为：
$$
\begin{align}
p(\mu|X^i)&=a\prod_{k=1}^Np(x_k|\mu)p(\mu)\\
&=a\prod_{k=1}^N\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac 1 2 (\frac{x_k-\mu}{\sigma})^2\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac 1 2 (\frac{\mu-\mu_0}{\sigma} )^2))\\
&=a^*\exp(-\frac 1 2(\sum_{k=1}^N(\frac{x_k-\mu}{\sigma})^2+(\frac{\mu-\mu_0}{\sigma} )^2))\\
&=a^{**}
\end{align}
$$


#### 非监督参数估计

在未知样本类别的条件下的参数估计称为**非监督参数估计**

几个基本假设

- 样本来自类数为$c$的各类中，但不知道每个样本究竟来自于哪一类
- 每类的先验概率$p(\omega_j)$已知
- 类条件概率密度的形式$p(x|\omega_j,\theta_j)$已知
- 未知的只是$c$个参数向量$\theta_1,\theta_2,\dots,\theta_c$的值

似然函数：
$$
l(\theta)=p(\Re|\theta)
$$

## 线性判别函数

### 引言

基于样本的Bayes分类器：通过估计类条件概率密度，设计相应判别函数

如果可以估计概率密度函数，则可以使用贝叶斯决策来最优的实现分类

从样本数据中直接估计参数->根据对样本/问题的理解直接设定判别函数形式，直接求解！

![image-20211011155524302](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20211011155524302.png)

#### 参数方法

概率密度函数已知，训练样本去估计参数

#### 线性判别方法

与非参数方法类似，与参数没有直接关系。次优方法。

#### 基于样本的直接确定判别函数的方法

1. 针对不同情况，使用不同准则函数，设计出满足这些不同准则要求的分类器。
2. 这些准则的“最优”并不一定与错误率最小相一致：次优分类器。

### 线性判别函数的基本概念

d维空间中线性判别函数的一般形式：
$$
g(x) = w^Tx+w_0
$$
其中：$x$是样本向量-样本在$d$维特征空间中的描述，$w$是权向量，$w_0$是一个常数。

令$g(x) = g(x_1)-g(x_2)$，$g(x) = 0$定义了一个决策面，分开了两类。

判别函数$g(x)$也可以看作是特征空间中某个点$x$到超平面距离的一种代数度量。可定义：
$$
x=x_p+r\frac{w}{||w||}
$$
其中$x_p$是$x$到$H$的投影向量，$r$是$x$到$H$的垂直距离。

则
$$
\begin{align}
g(x) &=w^T(x_p+r\frac{w}{||w||})+w_0\\
&=w^Tx_p+w_0+r\frac{w^Tw}{||w||}\\
&=r||w||
\end{align}
$$
特殊情况：x为原点，则$g(x)=w_0$。

#### 设计分类器的主要步骤

1. 有一组具有类别标志的样本集
2. 根据实际情况确定一个准则函数 $J$,满足：$ J$是样本集和$w,wo ,a$的函数；$J$的值能反映分类器的性能，它的极值解对应于 “最好”的决策
3. 利用最优化方法求出准则函数的极值解和$w,wo ,a$，进而得到$g(x)$

### Fisher线性判别分析

#### 基本思想

希望投影后的一维数据满足：

1. 两类之间尽可能远
2. 每一类自身尽可能紧凑

即：用投影后数据的统计性质（均值和离散度）的函数作为判别优劣的标准

#### 定义符号

$m_1,m_2$两类数据均值向量

$S_1,S_2$两类数据离散度矩阵

$\mu_1,\mu_2$两类数据投影后一维数据均值

$\sigma_1,\sigma_2$两类数据投影后一维数据离散度
$$
m_i=\frac 1 N\sum^i x
$$

$$
S_i=\sum^i((x-m_i)(x-m_i)^T)
$$
则有：
$$
\begin{align}
\mu_i &= w^Tm_i\\
\sigma^2_i&=\sum(w^Tx-\mu_i)^2\\
&=w^T\sum(x-m_i)(x-m_i)^Tw\\
&=w^TS_iw
\end{align}
$$

#### Fisher准则函数

$$
J_F(w)=\frac{(\mu_1-\mu_2)^2}{\sigma_1^2+\sigma_2^2}\\
w_{opt} = \arg\max J_F(w)
$$

称$S_b=(m_1-m_2)(m_1-m_2)^T$类间离散度矩阵

称$S_t=S_1+S_2$类内总离散度矩阵

则：
$$
J_F(w)=\frac{w^TS_bw}{w^TS_tw}
$$

#### Fisher准则合理性

$J_F(w)$只与投影方向有关，与$||w||$无关，若$w$是最优解，则$\lambda w$也是最优解。

#### Fisher最佳投影方向求解

需要$S_t=S_1+S_2$正定，否则存在$w$使得$w^TS_tw=0$，导致$J_F(w)$无极大值。

又因为有界性，则$J_F(w)\le\frac{\max \lambda(S_b)}{\min \lambda(S_t)}$

$\lambda(S)$表示$S$的特征根。

又因为$S_t$正定，则存在最优的$w$，使得$w^TS_tw=1$

本来是无约束优化$\max\frac{w^TS_bw}{w^TS_tw}$，等价为带约束最优化：
$$
\max w^tS_bw\\
s.t.\space w^TS_tw=1
$$

$$
L(w,\lambda) = w^TS_bw-\lambda(w^TS_tw-1)\\
\frac{\part L(w,\lambda)}{\part w} = S_bw-\lambda S_tw =0
$$
由定义：
$$
(m_1-m_2)(m_1-m_2)^Tw_{opt}=\lambda S_tw_{opt}
$$
记$c=(m_1-m_2)^Tw_{opt}$，则：
$$
w_{opt}=\frac{c}{\lambda}S_t^{-1}(m_1-m_2)
$$
而我们只关心投影方向，所以：
$$
w_{opt}=S_t^{-1}(m_1-m_2)=(S_1+S_2)^{-1}(m_1-m_2)
$$
映射就此完成，现在确定分类阈值$w_0$，即两个样本投影后的加权平均值（权为样本个数）的负数

### 感知准则函数

为讨论方便，将$x$增加一维：$y=[1,x_1,x_2,\dots,x_d]^T$，增广的权向量为：$a=[w_0,w_1,w_2,\dots,w_d]^T$ ，线性判别函数为：$g(y)=a^Ty$

#### 基本概念

##### 线性可分性

训练样本集中的两类样本在特征空间可以用一个线性分界面正确无误的分开。

##### 规范化样本向量

将第二类样本取其反向向量。

![image-20211011164033943](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20211011164033943.png)

#### 感知准则函数

$$
J_P(a)=\sum_{y\in Y_M}(-a^Ty)
$$

式中$Y_M$是被$a$错误分类的实例集合，由于全部反向，导致所有错误分类的实例有$a^Ty<0$，因此$J_P(a)$非负。

定义梯度：
$$
\nabla_aJ_P(a)=\sum_{y\in Y_M}(-y)
$$
梯度衰减更新法则为：
$$
a_{k+1} = a_k+\eta_k\sum_{y\in Y_M(k)}y
$$
也被称为感知机批次更新法则。

![image-20211011164542002](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20211011164542002.png)

感知准则函数修正法：

1. 单样本修正法：样本集视为不断重复出现的序列，逐个样本检查，修正权向量
2. 批量样本修正法：样本成批或全部检查后，修正权向量。

#### 小结

感知准则函数方法思路：

1. 找初始向量$a_1$，用训练样本集中每个样本计算
2. 若发现某个$y$有$a^Ty<0$，则只要$a_{k+1} = a_k+\eta_ky$，则必有$a_{k+1}^Ty = a_k^Ty+\eta_ky^Ty$，有趋势使得$a_{k+1}^Ty>0$

当然，修正后的$a$也有可能使某些$y$出现$a_{k+1}^Ty<0$，但只要训练样本集线性可分，无论初值为什么，有限次迭代都能收敛。

### 最小平方误差准则函数

规范化增广样本向量$y_i$，增广权向量$a$，正确分类要求：$a^Ty_i>0$

本质就是求一组$N$个线性不等式的解。

样本集增广矩阵$Y$及一组$N$个线性不等式可由矩阵表示

引入余量$b=[b_1,b_2,\dots,b_N]^T$，$b_i$为任意给定正常数，$a^Ty_i=b_i>0$，则可表示为：
$$
Ya=b
$$
定义误差向量$e=Ya-b$，定义平方误差准则函数：
$$
J_S(a)=||e||^2=||Ya-b||^2=\sum_{i=1}^N(a^Ty_i-b_i)^2
$$
求解
$$
a^*=\arg\min_aJ_S(a)
$$

$$
\nabla J_S(a)\sum_{i=1}^N2(a^Ty_i-b_i)y_i=2Y^T(Ya-b)
$$

则：
$$
\nabla J_S(a^*)=0 \iff Y^TYa^*=Y^Tb\\
a^*=(Y^TY)^{-1}Y^Tb=Y^+b
$$

#### 与其他方法的关系

与Fisher：当$b=[N/N_1,N/N_1,\dots,N/N_1(N_1个),N/N_2,N/N_2,\dots,N/N_2(N_2个)]^T$

MSE等价于Fisher

与Bayes：当$N\to \infin,b=u_N$时，则它以最小均方误差逼近Bayes判别函数$g(x)=P(\omega_1|x)-P(\omega_2|x)$

#### MSE方法的迭代解

伪逆解计算量大，实际使用的梯度下降法
$$
\nabla J_S(a)\sum_{i=1}^N2(a^Ty_i-b_i)y_i=2Y^T(Ya-b)=\left\{\begin{align}&a_1随机初始化\\&a_{k+1}=a_k-\eta_kY^T(Ya-b) \end{align} \right.
$$
满足一定条件时梯度下降结束。

也可以使用单样本修正调整权向量：Widrow-Hoff/最小均方根/LMS算法
$$
a_{k+1} = a_k +\eta_k(b_i-a_K^Ty_i)y_i
$$
其中$y_i$是使得$a_K^Ty_i\ne b_i$的样本。

### 多类问题

三种方法：

#### c-1 二类问题

分离符合与不符合$\omega_1$的点

#### c(c-1)/2 二类问题

分离每一对

#### 定义c个线性离散函数

将$x$归于$w_j$类如果$g_i(x)>g_j(x),\forall j\ne i$
$$
g_i(x) = w^T_ix+w_{i0}
$$

## 非线性判别函数

### 引言

线性判别函数：简单实用，但线性不可分时错误率大。

1. 使用新的特征
2. 非线性变换
3. 非线性分类器

### 分段线性判别函数

- 决策面由若干超平面段组成，计算相对比较简单
- 能够逼近各种形状的超平面，适应性强
- 多类情况下的线性判别函数分类
- 树状分类

如果两类可划分为线性可分的若干子类，则可以设计多个线性分类器，实现分段线性分类器。

#### 基于距离的分段线性判别函数

分段线性距离分类器：将各类别划分为相对密集的子类，每个子类以它们的均值作为代表点，然后按最小距离分类。

判别函数：$\omega_i$有$l_i$个子类，将$\omega_i$的决策域$R_i$分成$l_i$个子域$R_i^1,R_i^2,\dots,R_i^{l_i}$，每个区域用均值$m_i^k$代表：
$$
g_i(x)=\min_{k=1,\dots,l_i}||x-m_i^k||
$$
判别规则：
$$
j=\arg\min_{i=1,\dots,c}g_i(x)
$$
如果$g_j(x)=\arg\min_{i=1,\dots,c}g_i(x)$，则$x$属于$\omega_j$。

#### 一般的分段线性判别函数

将每个大类分成若干子类，针对每个子类定义一个线性判别函数。

一般形式：$g_i^k(x)$表示第$i$类第$k$段线性判别函数，$l_i$是第$i$类所具有的判别函数个数，$w_i^k.w_{i_0}^k$扽别是第$k$段的权向量和阈值权：
$$
g_i^k(x)={w_i^{(k)}}^Tx+w_{i_0}^k
$$
第$i$类判别函数：
$$
g_i(x)-\max_{k=1,2,\dots,l_i}g_i^k(x)
$$
判别规则：如果$g_j(x)=\arg\min_{i=1,\dots,c}g_i(x)$，则$x$属于$\omega_j$。

决策面：$g_i^n(x)=g_j^m(x)$

问题：如何确定子类数目？如何求得各子类的线性判别函数？

### 分段线性判别函数的设计

#### 已知子类划分

直接使用多类线性分类方法

如何知道子类划分？**先验、聚类分析**

#### 只知道子类数目，不知道子类划分

使用错误修正法（此法介绍使用增广的判别函数形式表示$g_i^k(y)={a_i^{(k)}}^Ty$）

假设$\omega_i$，$\omega_i$类中有$l_i$个子类，每一类均存在一定数量训练样本：

1. 初始化：任意给定各类各子类权值${a_i^{(k)}}^T(0)$，通常使用小随机数
2. 在时刻$t$：当前权值为${a_i^{(k)}}^T(t)$，考虑某个训练样本$y_v\in\omega_j$，找出$\omega_j$类的子类中最大的判别函数${a_i^{(m)}}^T(t)y_v=\max_k{a_j^{(k)}}^T(t)y_v$
3. 考察当前权值对$y_v$的分类情况，若${a_j^{(m)}}^T(t)y_v>{a_i^{(k)}}^T(t)y_v,\forall i\ne j$，则${a_i^{(k)}}^T(t)$不变。若$\exist i\ne j,k=n,s.t.{a_j^{(m)}}^T(t)y_v\le{a_i^{(n)}}^T(t)y_v$，则$y_v$被错分，对其中最大者记为$(i^{'},n^{'})$修正：

$$
a_j^{(m)}(t+1) = a_j^{(m)}(t) + \rho_ty_t\\
a_{i^{'}}^{(n^{'})}(t+1) = a_{i^{'}}^{(n^{'})}(t) - \rho_ty_t
$$

重复迭代，直至收敛。式中$\rho$为自取步长

#### 未知子类数目

使用树状分段线性分类器。

#### 用凹函数的并来表示分段线性函数

设$l_i$为线性判别函数，则：

1. $l_1,l_2,\dots,l_r$都是分段线性判别函数
2. 若$A,B$都是分段线性判别函数，则：$A\wedge B,A\vee B$也是分段线性判别函数。
3. 对任何分段线性函数都可以表示为析取范式或合取范式。

析取范式中最小项$(L_{11}\wedge L_{22}\wedge\dots\wedge L_{1m})$称为**凹函数**。

对于多峰二类问题：设第一类有$q$个峰，则有$q$个凹函数，即：
$$
P=P_1\vee P_2 \vee \dots \vee P_q
$$
每个凹函数$P_i$由$m$个线性判别函数来构成：
$$
P_i=L_{i1}\wedge L_{i2}\wedge \dots \wedge L_{im}
$$
假设对于每个子类的线性判别函数$L_{ij}$都设计成：
$$
L_{ij}=w_{ij}x\left\{ \begin{aligned}&>0,x\in\omega_1,i=1,2,\dots,q子类\\&<0,x\in\omega_2,j=1,2,\dots,m每个子类的判别函数数\end{aligned}\right.
$$
最终判别规则：
$$
P>0,则x\in\omega_1\\P\le0,则x\in\omega_2
$$
例：

![image-20211018165243575](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20211018165243575.png)

$\omega_1$分三个峰，$q=3$。

判别函数：
$$
m_1=5,m_2=4,m_3=4
$$
所以共有十三个分段判别函数
$$
\begin{align}
P&=(L_{11}\wedge L_{12}\wedge L_{13}\wedge L_{14}\wedge L_{15})\vee(L_{21}\wedge L_{22}\wedge L_{23}\wedge L_{24})\vee(L_{31}\wedge L_{32}\wedge L_{33}\wedge L_{34})\\
&=\max(\min(l_{11},l_{12},l_{13},l_{14},l_{15}),\min(l_{21},l_{22},l_{23},l_{24}),\min(l_{31},l_{32},l_{33},l_{34}))
\end{align}
$$
若$P>0$则$x\in\omega_1$，否则$x\in \omega_2$

#### 用交遇区的样本设计分段线性函数

思想：寻找两类中最靠近的样本子集，用他们设计分类器。

步骤：

1. 用聚类分析等方法把每类样本分为若干子类
2. 考察子类间的距离$d(v_i^m,v_j^n)$
3. 寻找紧互对原型对$d(v_i^m,v_j^n)=\min_l(d(v_i^l,v_j^n))=\min_l(d(v_i^m,v_j^l))$
4. 用紧互对原型对设计分类面
5. 决策规则（可能有错分）：设最后得到$m$个超平面$H_i:\alpha _i^Ty=0$，记$z_i(x)=if\ \alpha _i^Ty>0: 1,\ else\ 0$，得$z(x)=[z_1(x),z_2(x),\dots,z_m(x)]$，对$z(x)$的每一种可能取值$z_j$，统计其在$\chi_1,\chi_2$两类样本中出现的次数$N_1(z_j),N_2(z_j)$，定义$\Omega(z_j)$：若$N_1(z_j),N_2(z_j)$很小，则$\Omega(z_j)=\delta$，否则若$L=\frac {N_1(z_j)}{N_1(z_j)+N_2(z_j)}>1$，则$\Omega(z_j)=1$，否则若$L<2$，$\Omega(z_j)=0$。
6. 最终决策规则：对输入$x$，若$\Omega(z_j)=1$则$x\in \omega_1$；若$\Omega(z_j)=0$，则$x\in\omega_2$；若$\Omega(z_j)=\delta$则拒绝。

### 二次判别函数

二次判别函数一般表示为：
$$
\begin{align}
g(x)&=X^T\bar WX+W^TX+W_0\\
&=\sum_{i=1}^nw_{ii}x_i^2+2\sum_{j=1}^{n-1}\sum_{i=j+1}^nw_{ji}x_jx_i+\sum_{j=1}^nw_jx_j+W_0
\end{align}
$$
其中：$\bar W$是$n\times n$维权向量，$W$为$n$维权向量，系数共有$l=\frac 1 2n(n+3)+1$（很大）

二次决策面为超二次曲面。

#### 已知样本$\omega_1$集中，而$\omega_2$分散

定义$\omega_1$判别函数：
$$
g(x) = k^2-(x-\bar \mu)^T\Sigma^{-1}(x-\bar \mu)
$$
$k$决定超平面大小，$\mu$为$\omega_1$均值，$\Sigma$为$\omega_1$协方差。

#### 已知样本$\omega_1,\omega_2$都集中

可以定义两个判别函数：
$$
g_i(x) = k_i^2-(x-\bar \mu_i)^T\Sigma_i^{-1}(x-\bar \mu_i)
$$
$\mu_i$为$\omega_i$均值，$\Sigma_i$为$\omega_i$协方差。

判别面方程：
$$
g(x)=g_1(x)-g_2(x) = 0
$$

### 神经网络综述

#### 简介

神经网是一种模拟动物神经网络行为特征，进行分布式并行信息处理的算法。

这种网络依靠系统的复杂程度，通过调整内部大量节点之间相互连接的关系，从而达到处理信息的目的。

基础：

1. 构成：大量简单的基本元件——神经元
2. 工作原理：模拟生物神经处理信息
3. 功能：进行信息的并行处理和非线性转换

特点：

1. 比较轻松地实现非线性映射
2. 具有大规模的计算能力

#### 发展

1. MP模型
2. Hebb学习规则
3. 感知机模型
4. 感知器，局限性
5. 能量函数
6. 反向传播，解决多层前向神经网络学习问题

#### 优劣势

优势：

1. 很强的自适应学习能力
2. 实现特征空间较复杂的划分
3. 能用高速并行处理系统实现

劣势：

1. 需要更多训练数据
2. 在普通计算机模拟运行速度较慢
3. 无法解释

#### 本质

利用计算机语言模拟人脑做决定

#### 神经元结构模型

![image-20211122164223979](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20211122164223979.png)

#### 几种代表性的网络模型

1. 单层前向神经网络——线性网络
2. 阶跃网络
3. 多层前向神经网络（反推学习规则，即BP神经网络）
4. Elman、Hopfield、双向联想记忆网络、自组织竞争网络

#### 神经网络能干什么

函数逼近、数据聚类、模式分类、优化计算等。

### BP神经网络

BP 神经网络是一种按**误差逆传播算法(反向传播）**训练的多层前馈网络，是应用广泛的神经网络模型之一。

BP网络能学习和存贮大量的输入-输出模式映射关系，而无需事前揭示描述这种映射关系的数学方程。

学习规则是使用最速下降法，通过反向传播来不断调整网络的权值和阈值，使网络的误差平方和最小。

BP神经网络模型拓扑结构包括输入层(input layer)隐层(hidden layer)和输出层(output layer)

#### 特点

多层前馈神经网络，信号向前传播，误差向后传播。

#### 工作原理

基本BP算法包括两个方面：信号的前向传播和误差的反向传播。即计算实际输出时按从输入到输出的方向进行，而权值和阈值的修正从输出到输入的方向进行。利用输出后的误差来估计输出层的直接前一层的误差，再 用这个误差估计更前一层的误差，如此反传下去，就获得了所有其他各层的误差估计。

#### 工作流程

1. 网络初始化：根据训练数据确定网络输入神经元、隐含神经元、输出神经元数目，初始化各层神经元间的连接权值，初始化隐含层、输出层的阈值，给定学习率和神经元传递函数。
2. 隐含层输出计算：根据输入向量、输入层和隐含层连接权值、阈值，计算隐含层输出
3. 输出层输出计算：根据隐含层输出、连接权值、阈值，计算BP神经网络预测输出
4. 误差计算：根据预测输出和期望输出计算网络预测误差
5. 权值更新：根据网络误差更新网络权值
6. 阈值更新：根据网络误差更新
7. 判断是否结束

#### 学习过程

正向传播：

输入样本$\to$输入层$\to$隐含层$\to$输出层

判断是否转入反向传播阶段：若输出层的实际输出与期望的输出不符。

误差反传：误差以某种形式在各层表示——修正各层单元的权值。

#### 缺陷与改进

缺陷：

1. 学习效率低，收敛速度慢
2. 易陷入局部极小状态

改进：

隐含层层数：过少则误差大，过多则会过拟合，学习时间增长。

隐含层神经元数量一般使用经验判断，$m$为隐含层神经元数，$n$为输入层神经元数。$I$为输出层神经元数，$\alpha$为$1-10$间的常数，则：$m = \sqrt{n+l}+a,m=\log_2 n,m=\sqrt{nl}$ 等。

#### 其它常用传统神经网络模型

##### 径向基函数网络

特点：**只有一个隐层**，隐层单元采用径向基函数作为输出函数，输入层到隐层的权值固定为1，输出节点为线性求和单元，隐层到输出节点间权值可调。

**径向基函数**

某种沿径向对称的标量函数。通常定义为空间中任一点$x$到某一中心$x_c$之间欧氏距离的单调函数，可记作$k(||x-x_c||)$。

最常用的是高斯核函数：$k(||x-x_c||)=\exp\left\{-\frac{||x-x_c||^2}{2\delta^2}\right\}$

**作用**

把网络看作对未知函数的逼近器，输入层到隐层的基函数是非线性的，而输出是线性的，可看作先对原始非线性可分特征空间变换到另一空间，通过这一变换使得在新空间线性可分，再用线性单元解决问题。

**可调参数**

隐层基函数中心、方差，输出单元权值。

##### Hopfield 网络

Hopfield网络是一种反馈网络。反馈网络具有一般非线性系统的许多性质，如稳定性问题、各种类型的吸引子以及混沌现象等。它比前馈网络的内容复杂。

Hopfield还满足：

1. 权值对称：权矩阵为对称阵
2. 无自反馈：权矩阵对角线元素为0

##### 自适应共振理论神经网络ART

自适应共振理论神经网络既能模拟人脑的可塑性，可以学习知识，又能模拟人脑的稳定性，学习新的知识但不破坏原有知识，即这种网络**不仅能记忆新的知识，而且还保留已记忆的内容**。

##### 自组织特征映射神经网络SOM

人脑的记忆不是神经元与记忆模式的一一对应，而是一群神经元对应一个模式。

## 支持向量机

回顾：

- 线性判别函数
- Fisher线性判别函数
- 感知准则函数
- 最小平方误差准则函数
- 多类问题

### 线性支持向量机

SVM从线性可分情况下的最优分类面发展而来。

**最优分类面**就是要求分类线不仅能将两类正确分开，并且**分类间隔最大**。

SVM考虑找到一个超平面，并且使训练集中的点距离分类面尽可能远，也就是使其两侧的空白区域（margin）尽可能大。

样本集：$\{x_n.t_n\},n=1,2,\dots,N,x_n\in\mathbb R^d;t_n\in\{-1,1\}$

分类器：$y(x) = w^{T}x+b$

其中：
$$
t_n=\left\{ \begin{aligned}
	1,y(x_n) > 0&\ \mathrm {if}\ x_i\in w_1\\
    -1,y(x_n) < 0&\ \mathrm {if}\ x_i\in w_2\\
\end{aligned}
\right.
$$
即：
$$
t_ny(x_n)>0
$$
样本集任意一点$x_n$到分类面的距离：
$$
\frac{t_ny(x_n)}{||w||}=\frac{t_n(w^Tx_n+b)}{||w||}
$$
优化$w,b$使得margin最大。

求解很复杂，可以令超平面最近的点有：$t_n(w^{T}x_n+b) = 1$，则所有点都有：$t_n(w^{T}x_n+b) \ge 1$

这时只需要最大化$||w||^{-1}$，等价于：
$$
\arg\min_{w,b}\frac 1 2 ||w||^2\\
s.t.\quad t_i[(w^Tx_i)+b]\ge1
$$
转化为二次规划问题。

拉格朗日乘子法求解即可。

### 非线性支持向量机

对于非线性可分的数据样本，可通过适当的函数变换，将其在高维空间中转化为线性可分。

例如异或问题：

二维样本集：$x = (x_1.x_2)$

第一类$(0,0),(1,1)$，第二类$(1,0),(0,1)$

映射函数$\phi(x) = (x_1,x_2,x_1x_2)$

则：$(0,0)\to (0,0,0),(1,1)\to (1,1,1)$,$(1,0)\to (1,0,0),(0,1)\to (0,1,0)$

线性可分了。即将$y(x)=w^Tx+b$转化为$y(x)=w^T\phi(x)+b$。

#### 核函数

决策时：原本为
$$
y(x)=w^Tx+b=\sum_{n=1}^Na_nt_nx^Tx_n+b
$$
转化为
$$
y(x)=\sum_{n=1}^Na_nt_nK(x,x_n)+b
$$
其中：
$$
K(x,x_n)=\phi(x)^T\phi(x_n)
$$
核函数在特征空间中直接计算数据映射后的内积，简化了计算过程。

基于泛函理论，我们只需要知道核函数，没必要知道非线性变换的表达形式。

核函数需要满足**Mercer定理**：

$\mathbb R ^ n\times \mathbb R ^ n\to \mathbb R$上的映射$k$是一个有效核函数，当且仅当对于训练样本其相应的核函数矩阵是对称半正定的。即对于任何平方可积函数$g(x)$，有$\iint k(x,y)g(x)g(y)\mathrm dx\mathrm dy\ge 0$。

根据问题和数据不同，选择不同的核函数，如：

- 线性核：$k(x_1,x_2)=x_1^Tx_2$
- 多项式核：$k(x_1,x_2)=(<x_1,x_2>+R)^d$
- 高斯核：$k(x_1,x_2)=\exp\left\{-\frac{||x_1-x_2||^2}{2\sigma^2}\right\}$
- sigmoid核：$k(x_1,x_2) = \tanh(\beta_0x_1^Tx_2+\beta_1)$

#### 非线性SVM基本思想

1. 通过非线性变换将输入空间变换到高维空间。
2. 在新空间中求最优分类面。

非线性变换通过定义适当的内积核函数实现。而选择不同的核函数，可以看作是选择不同的相似性度量。线性支持向量机就是采用欧氏空间中的内积作为相似性度量。

### 其他非线性分类方法

#### 决策树

内部节点上选用一个属性进行分割，分支表示输出，叶子节点表示类。

如：

![image-20211129190521830](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20211129190521830.png)

决策树算法对数据处理的过程中，将数据按树状结构分成若干分支形成决策树，从根到树叶的每条路径创建一个规则。

但是，也有很烂的决策树，如：

![image-20211129190653328](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20211129190653328.png)

怎样让决策树深度没那么深？需要对决策树生成步骤进行改变。

#### 决策树的生成

1. 在条件属性集中选择最有分类标识能力的属性作为决策树当前节点。
2. 根据当前决策属性取值不同，将训练样本数据集划分为若干子集。
3. 针对上一步得到每一个子集，重复上述过程，直到子集中所有元组都属于同一类，不能再进一步划分为止。

#### 决策树分类算法

ID3算法。

**基本思想**：按一定准则选择一个条件属性作为根节点，根据其属性取值将整个例子空间划分为几个子空间，然后递归使用这一准则继续划分，直至所有底层子空间只含有一类例子。

一些数学定义：

##### 熵

度量样例的纯度。

定义：设$S$是$n$个数据样本的集合，将样本划分为$c$个不同的类，每个类含样本数$n_i$，则$S$划分为$c$个类的熵为：
$$
\mathrm {Entropy}(S)=-\sum_{i = 1}^c\frac{n_i}{n}\log_2\frac{n_i}{n}
$$

##### 信息增益

衡量属性区分训练样例的能力：一个属性的信息增益就是由于**使用这个属性**分割样例而**导致的熵的降低**。

属性$A$相对样例集合$S$的信息增益定义：
$$
\mathrm {Gain}(S,A) = \mathrm {Entropy}(S) - \sum_{v\in \mathrm{Values}(A)}\frac{|S_v|}{|S|}\mathrm {Entropy}(S)
$$
则ID3算法每次选择信息增益值最大的属性作为最佳属性，进行分类。

#### 其它决策树建立方法

CART算法（使用基尼指数）、C4.5算法（使用信息增益率）等。

#### 决策树的数据准备

- 数据清理：删除、减少噪声，补填空缺值。
- 相关性分析： 对于问题无关的属性或者不能归纳的属性，直接删除。
- 数据变换：数据标准化，数据归纳，并控制属性可能值不超过七种。

#### 剪枝

先剪枝：

- 数据划分法
- 阈值法
- 信息增益的统计显著性分析

后剪枝：

- 减少分类错误修剪法
- 最小代价与复杂性折中
- 最小描述长度准则

#### 随机森林

决策树方法尤其受数据集影响大，容易过学习。

统计学策略：自举（bootstrap）通过对现有样本重采样形成多个样本集，模拟数据中的随机性。

随机森林步骤：

1. 对样本数据进行自举重采样
2. 用每个重采样样本集作为训练样本构造一个决策树
3. 得到所需数据的决策后，对这些树的输出进行决策，得票最多的类作为随机森林的决策。

## 特征选择与提取

## 图像特征提取

## 深度学习

## 非监督学习方法

### 引言

- 监督学习（supervised learning）：用已知类别的样本训练分类器，以求对训练集数据达到某种最优，并能推广到对新数据的分类 
- 非监督学习（unsupervised learning）：样本数据类别未知，需要根据样本间的相似性对样本集进行分类（聚类，clustering

监督学习方法必须要有训练集与测试样本。在训练集中找规律，而对测试样本使用这种规律；而非监督学习只有一组数据，在该组数据集内寻找规律。

监督学习方法的目的是识别事物，给待识别数据加上标注。而非监督学习方法只有要分析的数据集本身，没有标注。

### 主要的非监督学习方法

- 基于概率密度函数估计的直接方法：设法找到各类别在特征空间的分布参数再进行分类，如直方图方法。
- 基于样本间相似性度量的间接聚类方法：设法定出不同类别的核心或初始类核，然后依据样本与这些核心之间的相似性度量将样本聚集成不同类别。

#### 基于概率密度函数估计的直接方法

划分整个空间为 $N$ 个区域，使得每个区域的概率密度函数是单峰的。

![image-20211229101825612](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20211229101825612.png)

##### 单峰子集的分离方法

思想：把特征空间分为若干区域，每个区域上，混合概率密度函数都是单峰的，每个单峰对应一个类别。

一维空间：应用直方图或 Parzen 窗法估计概率密度函数，找到概率密度函数的峰，以及峰之间的谷底，以谷底为阈值对数据进行分割。

即：$t = \arg\min p(x)$

##### 多维空间投影方法

多维空间中，直接划分比较困难，一般投影到一维空间中来简化问题。

如何确定合适的投影方向 $\mathbf u$ ？

使投影 $\mathbf x = \mathbf{u^T y}$ 的方差最大，方差越大，类之间的分离程度也可能越大，样本协方差矩阵的最大特征值对应的特征向量满足这样的要求，但有时不能产生多峰的边缘密度函数。

投影算法步骤：

1. 计算样本 $\mathbf y$ 协方差矩阵的最大特征值对应的特征向量 $\mathbf u$，把样本数据投影到 $\mathbf u$ 上，得到 $\mathbf v = \mathbf{u^T y}$
2. 用直方图/ Parzen 窗法求边缘概率密度函数 $p(v)$
3. 找到边缘概率密度函数的各个谷点，在这些谷点上作垂直于 $\mathbf u$ 的超平面把数据划分成几个子集
4. 若无谷点，则用下一个最大的特征值代替
5. 对所得到的各个子集进行同样的过程，直至最后都是单峰为止

但有时并不能一定分离。可以利用迭代算法。

可以将数据集 $Y$ 划分为 $c$ 个子集 $\Gamma_i$ ，每个子集中样本数为 $N_i$ ,总数为 $N$ ，考察类条件概率密度的加权估计值：
$$
f(y|\Gamma_i) = \frac{N_i}N p(y|\Gamma_i)
$$
定义指标：
$$
J=\frac 1 2 \int \sum_{i=1}^c\sum_{j=1}^c\left[f(y|\Gamma_i)-f(y|\Gamma_j) \right]^2p(y)\mathrm dy
$$
反映了两个类间的距离，目标：求使 $J$ 最大的子集划分：
$$
f(y|\Gamma_i)=\frac{1}{N_i}\sum_{j=1}^{N_i}k(y,y_i),y\in \Gamma_i
$$
即 Parzen 窗法。

算法：

1. 初始划分 $Y$
2. 对每个样本 $y_k$ ，逐一计算 $f(y_k|\Gamma_i)$ ，并归入使 $f(y_k|\Gamma_i)$ 最大的子集中
3. 重复2，直到不在有样本发生转移

#### 基于相似性度量的间接聚类方法

根据样本间的相似性，使某种准则函数最大（小）

例如 K 均值，使下列准则最小：
$$
J=\sum_{i=1}^{K}\sum_{y\in\Gamma_i}||y-m_i||^2
$$
式中： $m_i$ 为第 $i$ 类个聚类 $\Gamma_i$ 的样本均值。

目标：类内元素相似性高，类间元素相似性低

要点：**相似性度量、准则函数**

##### 相似性度量

以某种距离定义，如样本间相似性度量（特征空间的某种距离度量）：
$$
\delta(x_i,x_j) = (x_i-x_j)^T (x_i-x_j)
$$
样本与样本聚类间的相似性度量：
$$
\delta(x_i,K_j)
$$
直观理解：同一类的样本的特征向量应是相互靠近的。 

#### 动态聚类方法

- 距离函数：进行相似性度量
- 准则函数：评价聚类结果的质量
- 迭代，直到准则函数取得极值

### K 均值算法

给定 $D$ 维空间上的数据集 $\{x_1,\dots,x_N\}$，并不知道这些数据集对应的类型和标号，通过聚类方法将这些数据集划分为 $K$ 类，对于 $K$ 个聚类的每一类 $k$ ，分别建立一个代表点 $\mu_k$ ，将每一类样本划归到离该样本最近的 $\mu_k$ 代表的聚类。

目的：最小化准则函数 $J=\sum_{n=1}^N\sum_{k=1}^Kr_{nk}||x_n-\mu_k||^2$。式中 $r_{nk}$ 表示 $x_n$ 是否是第 $k$ 个聚类。

算法：

1. 根据 $\mu_k$ 按最优化准则计算 $r_{nk}$ 。
2. 根据 $r_{nk}$ 按最优化准则计算 $\mu_k$ 。

#### K 均值聚类方法用于解决的问题

1. 分类数已知
2. 是最小方差划分，并不一定能反映内在分布
3. 与初始划分有关，不保证最优

#### K 均值的缺点

用均值代表类，适用于近似球状分布的类

改进：用核来代表类，定义样本与核的相似性度量，如正态核函数。

#### 分级聚类方法

在人类认识客观世界过程中，将事物分级分类是一种很有效的手段，如界门纲目科属种。

聚类划分序列： $N$ 个样本自底向上逐步合并成一类。

1. 初始化，每个样本自成一类
2. $K$ 水平划分的进行：计算已有的 $c=N-K+1$ 个类的类间距离矩阵 $D^{(K-1)} = [d_{ij}]^{(K-1)}$，其最小元素记作 $d^{(K=1)}$，相应的两个类合并成一类
3. 重复 2 ，直至形成包含所有样本的类。

# Pic。

#### 两聚类间的距离度量

聚类 $K_i$ 与 $K_j$ 间的距离度量：

最近距离、最远距离、均值距离。

### 非监督学习的一些问题

非监督学习存在更大不确定性：可利用信息少——相似性度量一般对于数据尺度较敏感。

影响聚类结果的因素：样本的分布、样本数量、聚类准则、相似性度量、预分类数等

针对不同数据、不同目标选择不同的聚类算法

动态聚类算法计算效率高，实际应用多

改进方法：

- 先验知识
- 多次试算
- 改进算法：自组织映射 SOM ，模糊 C 均值方法

