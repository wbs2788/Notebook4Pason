# 强化学习

## 马尔可夫决策过程

![image-20210917155234957](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20210917155234957.png)

马尔可夫决策过程可以建模真实世界的问题，被描述为强化学习框架。

### 马尔可夫性

状态历史：$h_t=\{s_1,s_2,\dots,s_t\}$

如果状态$s_t$是有马尔科夫性的当且仅当：
$$
p(s_{t+1}|s_t)=p(s_{t+1}|h_t)\\
p(s_{t+1}|s_t,a_t)=p(s_{t+1}|h_t,a_t)
$$

### 马尔可夫奖励过程

马尔科夫链+回报

#### 定义

- $s$是状态集
- $P$是动态转移模型，描述$P(S_{t+1}=s^{'}|s_t=s)$
- $R$是奖励函数，$R(s_t=s)=\mathbb E[r_t|s_t=s]$
- $\gamma$是衰减因子

#### 返回与价值函数

返回函数：计算从第t步到稳定状态的奖励和。
$$
G_t=R_{t+1}+\gamma R_{t+2} + \gamma^2 R_{t+3} + \dots
$$
更倾向于当前得到的奖励。

价值函数：求期望
$$
V_t(s)=\mathbb E[G_t|s_t=s]=\mathbb E[R_{t+1}+\gamma R_{t+2} + \gamma^2 R_{t+3} + \dots|s_t=s]
$$
可以写出矩阵形式：
$$
V=R+\gamma PV
$$

### 马尔可夫决策过程

#### 定义

- $s$是状态的有限集集
- $A$是动作的有限集
- $P^a$是动态转移模型，描述$P(S_{t+1}=s^{'}|s_t=s,a_t=a)$
- $R$是奖励函数，$R(s_t=s,a_t=a)=\mathbb E[r_t|s_t=s,a_t=a]$
- $\gamma$是衰减因子

#### MDP策略

##### 定义

$$
\pi(a|s)=P(a_t=a|s_t=s)
$$

策略是稳定的（时间独立），$\forall t>0,A_t\sim\pi(a|s)$

给定MDP和策略$\pi$，状态序列$S_1,S_2,\dots$是马尔可夫过程，状态与奖励序列$S_1,R_1,S_2,R_2,\dots$是马尔科夫奖励过程，其中：
$$
P^\pi (s^{'}|s)=\sum_{a\in A}\pi(a|s)P(s^{'}|s,a)\\
R^\pi(s) = \sum_{a\in A}\pi(a|s)R(s,a)
$$
![image-20210917161335856](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20210917161335856.png)

#### MDP价值函数

状态价值函数是期望从状态$s$开始得到的返回值。
$$
v^\pi(s)=\mathbb E_\pi[G_t|s_t=s]
$$
动作价值函数是从状态$s$，执行动作$a$，遵守策略$\pi$的期望返回值。
$$
q^\pi(s,a)=\mathbb E_\pi[G_t|s_t=s,A_t=a]
$$
两个函数的关系：
$$
v^\pi(s)=\sum_{a\in A}\pi(a|s)q^\pi(s,a)
$$

### Bellman 期望等式

$$
v^\pi(s)=\mathbb E_\pi[R_{t+1}+\gamma v^\pi(s_{t+1}|s_t=s)]
$$

$$
q^\pi(s,a)=\mathbb E_\pi[R_{t+1}+\gamma q^\pi(s_{t+1},A_{t+1}|s_t=s,A_t=a)]
$$

![image-20210917161953738](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20210917161953738.png)

## 蒙特卡洛方法

### 核心思想

1. 建设随机过程
2. 采样
3. 建立不同的估计

例子：估计$\pi$