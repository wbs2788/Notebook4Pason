# 机器学习

## L2

### 蒙特卡洛搜索

决策树的启发式搜索算法

没有评估函数？随机行动模拟游戏，在游戏最终得分，保持胜利记录

#### 1 选择

递归应用策略，直到找到叶节点。

![image-20210913190647685](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20210913190647685.png)

对于被检查的局面而言，他可能有三种可能：

1. 该节点所有可行动作都已经被拓展过

2. 该节点有可行动作还未被拓展过

3. 这个节点游戏已经结束了(例如已经连成五子的五子棋局面)

对于这三种可能：

1. 如果所有可行动作都已经被拓展过了，那么我们将使用UCB公式计算该节点所有子节点的UCB值，并找到值最大的一个子节点继续检查。反复向下迭代。
2. 如果被检查的局面依然存在没有被拓展的子节点(例如说某节点有20个可行动作，但是在搜索树中才创建了19个子节点)，那么我们认为这个节点就是本次迭代的的目标节点N，并找出N还未被拓展的动作A。执行步骤[2]
3. 如果被检查到的节点是一个游戏已经结束的节点。那么从该节点直接执行步骤{4]。

#### 2 拓展

进行一次模拟游戏

![image-20210913190945917](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20210913190945917.png)

在选择阶段结束时候，我们查找到了一个最迫切被拓展的节点N，以及他一个尚未拓展的动作A。在搜索树中创建一个新的节点Nn作为N的一个新子节点。Nn的局面就是节点N在执行了动作A之后的局面。

#### 3 模拟

![image-20210913191117255](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20210913191117255.png)

为了让Nn得到一个初始的评分。我们从Nn开始，让游戏随机进行，直到得到一个游戏结局，这个结局将作为Nn的初始评分。一般使用胜利/失败来作为评分，只有1或者0。

#### 4 反向传播

![image-20210913191126689](C:\Users\surafce book2\AppData\Roaming\Typora\typora-user-images\image-20210913191126689.png)

在Nn的模拟结束之后，它的父节点N以及从根节点到N的路径上的所有节点都会根据本次模拟的结果来添加自己的累计评分。如果在[1]的选择中直接发现了一个游戏结局的话，根据该结局来更新评分。每一次迭代都会拓展搜索树，随着迭代次数的增加，搜索树的规模也不断增加。当到了一定的迭代次数或者时间之后结束，选择根节点下最好的子节点作为本次决策的结果。

小结一下：

- 选择（Selection）：从根节点*R*开始，连续向下选择子节点至叶子节点*L*。下文将给出一种选择子节点的方法，让[游戏树](https://zh.wikipedia.org/wiki/遊戲樹)向最优的方向扩展，这是蒙特卡洛树搜索的精要所在。
- 扩展（Expansion）：除非任意一方的输赢使得游戏在L结束，否则创建一个或多个子节点并选取其中一个节点*C*。
- 仿真（Simulation）：在从节点*C*开始，用随机策略进行游戏，又称为playout或者rollout。
- 反向传播（Backpropagation）：使用随机游戏的结果，更新从*C*到*R*的路径上的节点信息。

#### UCT算法：树内选择策略

选择子结点的主要困难是：在较高平均胜率的移动后，在对深层次变型的利用和对少数模拟移动的探索，这二者中保持某种平衡。
$$
\frac{w_i}{n_i}+c\sqrt\frac{\ln t}{n_i}
$$
式中：

- $w_i$代表第$i$次移动后取胜的次数；
- $n_i$代表第{\displaystyle i}![i](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20)次移动后仿真的次数；
- $c$为探索参数—理论上等于$\sqrt 2$；在实际中通常可凭经验选择；
- $t$代表仿真总次数，等于所有$n_i$的和。

### 增强学习

#### Q学习

[Q学习 - 维基百科，自由的百科全书 (wikipedia.org)](https://zh.wikipedia.org/wiki/Q学习)

*Q*-学习最简单的实现方式就是将奖励值存储在一个表格（Q-table）中，但是这种方式受限于状态和动作空间的数目。
$$
Q(state,action) = R(state,action) + \gamma \max(Q(next\ state,all\ actions))
$$
式中Q为Q表格，R为初始reward矩阵。

### 贝叶斯学习

#### 贝叶斯公式

$$
p(h|D)=\frac{p(D|h)p(h)}{p(D)}
$$

D:训练数据

h:假设

$p(h|D)$:给定训练数据下，假设h的条件概率

$p(D)$:数据集D的先验概率

#### 最大似然估计

估计的参数是有但不知道是多少的，那我就假设这个概率分布，求这个参数。

似然函数：
$$
l(\theta) = p(x_1,x_2,\dots,x_N|\theta)=\prod_{k=1}^Np(x_k|\theta)
$$
最大似然估计：
$$
\hat \theta_{ML}=\arg\max_\theta l(\theta)
$$
求法：
$$
\nabla_\theta l(\theta)|_{\hat\theta_{ML}}=\sum_{k=1}^N\nabla_\theta p(x_k|\theta)|_{\hat\theta_{ML}}
$$

## L4

### 决策树

#### 决策树学习

##### 信息增益

信息熵是度量样本集合纯度最常用的指标。
$$
\mathrm{Ent}(D)=-\sum_{k=1}^{|\mathcal Y|}p_k\log_2p_k
$$
$\mathrm{Ent}(D)$值越小，则$D$的纯度越高。

假设离散属性$a$有$V$个可能取值$\{a^1,a^2,\dots,a^V\}$，若使用$a$对样本集$D$进行划分，就会产生$V$个分支节点，第$v$个分支节点包含$D$中所有属性值为$a^v$的样本，记为$D^v$，则可以计算$D^v$的信息熵，再给出权重$|D^v|/|D|$，则有信息增益：
$$
\mathrm {Gain}(D,a) = \mathrm{Ent}(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}\mathrm{Ent}(D^v)
$$
这里的概率基于最后的结果。

选择最大的信息增益，作为划分属性。

## 线性判别函数

决策面函数：可能很复杂，用线性分类器搞。

### 线性判别函数

$g(x)=w^Tx$

1. 收集样本$K=\{x_1,x_2,\dots,x_N\}$
2. 按需要确定准则函数，反应分类器性能$J(K,w)$
3. 用优化技术求准则函数极值解$w^*$，从而确定判别函数，完成设计

$$
w^*=\arg \max_w J(K,w)
$$

一般形式：$g(x)=w^Tx+w_0$

### Fisher线性判别

线性判别函数$g(x)=w^Tx$

- 样本向量$x$各分量的线性加权
- 样本向量$x$与权向量$w$点积
- 如果$||w||=1$，可看作投影

Fisher准则：找到最合适的投影轴，使两类样本在该轴上投影之间的距离尽可能远，而每类样本投影尽可能紧凑，从而使分类效果最佳

### 感知器准测

#### 线性可分性

训练样本集中的两类样本在特征空间可以用一个线性分界面正确分开。

在该条件下，有：
$$
if\ y\in\omega_1,w^Ty>0\\
if\ y\in\omega_2,w^Ty<0\\
$$
首先规范化：

如果$y\in\omega_2$，$y^{'} = -y$，其余不变。

则编程

### 支持向量机

